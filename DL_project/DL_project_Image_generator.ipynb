{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Name: Rediet Ferew\n","# ID: UGR/1415/12\n","> **This project aims to build a model for image generation specifically for anime face images. The animefacedataset -> [https://www.kaggle.com/datasets/splcher/animefacedataset/](http://) which is available on kaggle is utilized. The detail process is explained below. The number of epochs should have been high but despite my continuous attempts I have been interrputed by internet connection and I haven't been able to produce a robust model for image generation.**"]},{"cell_type":"markdown","metadata":{},"source":["# Custom Dataset Representation\n","**AnimeFaceDataset class is defined to represent a dataset of anime faces, utilizing a specified root directory and an optional transformation function. The images are converted to RGB format, and if transformations are specified, they are applied accordingly**"]},{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-03T09:29:08.324885Z","iopub.status.busy":"2024-02-03T09:29:08.324512Z","iopub.status.idle":"2024-02-03T09:29:08.333119Z","shell.execute_reply":"2024-02-03T09:29:08.332193Z","shell.execute_reply.started":"2024-02-03T09:29:08.324850Z"},"trusted":true},"outputs":[],"source":["#to manipulate file paths and list files in the kaggle directory\n","import os \n","#for image processing tasks\n","from PIL import Image \n","\n","#for handling datasets and creating data loaders for efficient batch processing during training.\n","from torch.utils.data import Dataset, DataLoader\n","\n","# for common image transformations\n","import torchvision.transforms as transforms\n","\n","# This class will be used to represent the dataset of anime faces.\n","class AnimeFaceDataset(Dataset):\n","    \n","    #constructor method for the AnimeFaceDataset class\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.file_list = os.listdir(root_dir)\n","    \n","    #returns the length of the dataset\n","    def __len__(self):\n","        return len(self.file_list)\n","    \n","    #used to get an item from the dataset given an index \n","    def __getitem__(self, idx):\n","        img_name = os.path.join(self.root_dir, self.file_list[idx])\n","        image = Image.open(img_name).convert(\"RGB\")\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image\n"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Loader"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T09:29:08.383656Z","iopub.status.busy":"2024-02-03T09:29:08.383385Z","iopub.status.idle":"2024-02-03T09:29:08.416844Z","shell.execute_reply":"2024-02-03T09:29:08.416145Z","shell.execute_reply.started":"2024-02-03T09:29:08.383634Z"},"trusted":true},"outputs":[],"source":["#creates an instance of the AnimeFaceDataset class, specifying the root directory where the images are located \n","dataset = AnimeFaceDataset(root_dir='/kaggle/input/animefacedataset/images', transform=transforms.Compose([\n","    # Resizes the image to have a height and width of 64 pixels\n","    transforms.Resize((64, 64)),\n","    # Converts the image to a PyTorch tensor\n","    transforms.ToTensor(),\n","    # Normalizes the tensor by subtracting the mean (0.5, 0.5, 0.5) and dividing by the standard deviation (0.5, 0.5, 0.5)\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Adjust normalization as needed\n","]))\n","#used to efficiently load and batch the data during training\n","dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Generator Model\n","**The generator takes a latent vector of size latent_size and transforms it through a series of transposed convolutional layers, gradually upsampling the input. The architecture includes batch normalization and ReLU activation functions to enhance training stability. The final layer produces an RGB image with three channels, and the Tanh activation function is applied to ensure output values are within the range [-1, 1].**"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T09:29:08.474334Z","iopub.status.busy":"2024-02-03T09:29:08.474053Z","iopub.status.idle":"2024-02-03T09:29:08.483225Z","shell.execute_reply":"2024-02-03T09:29:08.482183Z","shell.execute_reply.started":"2024-02-03T09:29:08.474311Z"},"trusted":true},"outputs":[],"source":["import torch # Imports the PyTorch library\n","import torch.nn as nn #Imports the neural network module from PyTorch\n","import torch.optim as optim #Imports the optimization module from PyTorch\n","import torchvision.utils as vutils #Imports utility functions from torchvision\n","import matplotlib.pyplot as plt # Imports the matplotlib library for creating plots and visualizations\n","\n","#Class that represents the generator component of a Generative Adversarial Network (GAN)\n","class Generator(nn.Module):\n","    def __init__(self, latent_size):\n","        super(Generator, self).__init__()\n","        self.main = nn.Sequential(\n","            #transforms the input latent vector of size latent_size into a 4x4 feature map with 512 channels\n","            nn.ConvTranspose2d(latent_size, 512, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            \n","            #transposed convolutional layer that upsamples the input to 256 channels\n","            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            \n","            #transposed convolutional layer that upsamples the input to 128 channels\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            \n","            #transposed convolutional layer that upsamples the input to 3 channels -> RGB\n","            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Discriminator Model\n","\n","**The discriminator processes input images through a series of convolutional layers, gradually reducing spatial dimensions and increasing the number of channels. Leaky ReLU activation functions and batch normalization enhance the model's ability to capture features and promote stable training. The final layer acts as a binary classifier, outputting a single-channel prediction indicating whether the input image is real or generated by the generator. The Sigmoid activation function is applied to produce probability scores.**"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T09:29:08.485922Z","iopub.status.busy":"2024-02-03T09:29:08.485642Z","iopub.status.idle":"2024-02-03T09:29:08.497770Z","shell.execute_reply":"2024-02-03T09:29:08.497026Z","shell.execute_reply.started":"2024-02-03T09:29:08.485899Z"},"trusted":true},"outputs":[],"source":["#class that represents the discriminator component of a Generative Adversarial Network (GAN)\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.main = nn.Sequential(\n","            #layer that scans through the input image to detect features\n","            nn.Conv2d(3, 128, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            #layer that increases the number of channels from 128 to 256\n","            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            \n","            #layer that increases the number of channels from 256 to 512\n","            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            \n","            # layer that reduces the number of channels to 1, producing a single-channel output \n","            # classifier layer -> whether the input image is real or fake\n","            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T11:14:18.989851Z","iopub.status.busy":"2024-02-03T11:14:18.989167Z","iopub.status.idle":"2024-02-03T11:14:18.995549Z","shell.execute_reply":"2024-02-03T11:14:18.994647Z","shell.execute_reply.started":"2024-02-03T11:14:18.989818Z"},"trusted":true},"outputs":[],"source":["# Set random seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Hyperparameters\n","latent_size = 100\n","lr = 0.0002\n","betas = (0.5, 0.999)\n","epochs = 10"]},{"cell_type":"markdown","metadata":{},"source":["# Training Process\n","**A Generative Adversarial Network (GAN) is trained on an animefacedataset dataset using PyTorch. The generator (netG) and discriminator (netD) models are initialized along with the Binary Cross Entropy Loss criterion and Adam optimizers. The training loop iterates through epochs, where the discriminator is trained on both real and fake data, and the generator is trained to generate realistic data to fool the discriminator. The progress, including discriminator and generator losses, is printed at intervals. Generated images are saved at the end of each epoch, providing a visual representation of the model's learning progress. The training process aims to improve the generator's ability to create realistic images that are indistinguishable from genuine data, while the discriminator evolves to effectively distinguish between real and fake samples.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T11:14:24.480518Z","iopub.status.busy":"2024-02-03T11:14:24.479658Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[0/10][0/994] Loss_D: 1.4130 Loss_G: 4.3682 D(x): 0.4881 D(G(z)): 0.4851 / 0.0134\n","[0/10][100/994] Loss_D: 0.0135 Loss_G: 6.8106 D(x): 0.9928 D(G(z)): 0.0061 / 0.0013\n","[0/10][200/994] Loss_D: 0.0048 Loss_G: 6.6872 D(x): 0.9979 D(G(z)): 0.0027 / 0.0014\n"]}],"source":["# Initialize generator model\n","netG = Generator(latent_size)\n","# Initialize discriminator model\n","netD = Discriminator()\n","\n","#Binary Cross Entropy Loss\n","criterion = nn.BCELoss()\n","#Adam optimizer \n","optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=betas)\n","optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=betas)\n","\n","# Training loop\n","for epoch in range(epochs):\n","    for i, data in enumerate(dataloader, 0):\n","        #Fetches a batch of real images\n","        real_data = data\n","        batch_size = real_data.size(0)\n","        \n","        # Creates a tensor filled with 1.0 to represent real labels\n","        real_label = torch.full((batch_size, 1, 1, 1), 1.0)\n","        # Creates a tensor filled with 0.0 to represent fake labels\n","        fake_label = torch.full((batch_size, 1, 1, 1), 0.0)\n","\n","        # Clears the gradients of the Discriminator before computing gradients.\n","        netD.zero_grad()\n","        # Passes real data through the Discriminator\n","        output = netD(real_data)\n","        # Calculates the loss for real data\n","        errD_real = criterion(output, real_label.expand_as(output))  \n","        # Backpropagates the gradients\n","        errD_real.backward()\n","        # Computes the mean output\n","        D_x = output.mean().item()\n","\n","\n","        # Generates random noise\n","        noise = torch.randn(batch_size, latent_size, 1, 1)\n","        # Passes fake data through the Discriminator\n","        fake_data = netG(noise)\n","        output = netD(fake_data.detach())\n","        \n","        # Calculates the loss for fake data\n","        errD_fake = criterion(output, fake_label)\n","        # Backpropagates the gradients\n","        errD_fake.backward()\n","        # Computes the mean output for fake data\n","        D_G_z1 = output.mean().item()\n","        # Combines the losses for real and fake data\n","        errD = errD_real + errD_fake\n","        # Updates the Discriminator's parameters\n","        optimizerD.step()\n","\n","        # Clears the gradients of the Generator before computing gradients\n","        netG.zero_grad()\n","        # Passes fake data through the Discriminator\n","        output = netD(fake_data)\n","        # Calculates the Generator's loss\n","        errG = criterion(output, real_label)\n","        # Backpropagates the gradients through the Generator\n","        errG.backward()\n","        # Computes the mean output for the fake data through the updated Discriminator\n","        D_G_z2 = output.mean().item()\n","        # Updates the Generator's parameters\n","        optimizerG.step()\n","\n","        if i % 100 == 0:\n","            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n","                  % (epoch, epochs, i, len(dataloader),\n","                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n","\n","    # SPrints the discriminator and generator losses, and the mean outputs for real and fake data, at specified intervals\n","    with torch.no_grad():\n","        fake = netG(torch.randn(64, latent_size, 1, 1))\n","        vutils.save_image(fake, f'fake_samples_epoch_{epoch + 1}.png', normalize=True)\n","\n","# Visualize generated images\n","img_list = [f'fake_samples_epoch_{i + 1}.png' for i in range(epochs)]\n","\n","# Plot the generated images\n","fig, axs = plt.subplots(1, epochs, figsize=(epochs * 2, 2))\n","for epoch, ax in zip(range(epochs), axs):\n","    img = plt.imread(img_list[epoch])\n","    ax.imshow(img)\n","    ax.axis('off')\n","plt.show()\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T11:10:56.801103Z","iopub.status.busy":"2024-02-03T11:10:56.800695Z","iopub.status.idle":"2024-02-03T11:10:56.826087Z","shell.execute_reply":"2024-02-03T11:10:56.825275Z","shell.execute_reply.started":"2024-02-03T11:10:56.801072Z"},"trusted":true},"outputs":[],"source":["torch.save(netG.state_dict(), 'gan_model.pth')"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T11:11:53.258653Z","iopub.status.busy":"2024-02-03T11:11:53.257914Z","iopub.status.idle":"2024-02-03T11:11:53.302519Z","shell.execute_reply":"2024-02-03T11:11:53.301524Z","shell.execute_reply.started":"2024-02-03T11:11:53.258624Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["netG = Generator(latent_size)\n","netG.load_state_dict(torch.load('gan_model.pth'))"]},{"cell_type":"markdown","metadata":{},"source":["# Sample Generation"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-02-03T11:13:17.759629Z","iopub.status.busy":"2024-02-03T11:13:17.758839Z","iopub.status.idle":"2024-02-03T11:13:17.823815Z","shell.execute_reply":"2024-02-03T11:13:17.822965Z","shell.execute_reply.started":"2024-02-03T11:13:17.759589Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","with torch.no_grad():\n","    random_noise = torch.randn(1, latent_size, 1, 1)\n","    generated_image = netG(random_noise).squeeze().cpu().numpy()\n","    generated_image = (generated_image + 1) / 2.0  # Unnormalize the image\n","\n","    \n","    generated_image = generated_image.transpose(1, 2, 0)\n","\n","    plt.imshow(generated_image)\n","    plt.axis('off')\n","    plt.show()\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":379764,"sourceId":737475,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
